#### ImageAnalysis for Ecological Reserver Computing
#### OpenCV with Python BW_v4
#### 2020.12.16 Ushio
#### 2020.12.24, revised for grid-based cell counting, Ushio
#### 2020.12.24, revised for NN distance calculation
####             (grid-based counting removed), Ushio
#### 2021.3.19, v7: More accurate counting for low abundance image
####              : Disabled calculating NN distance between cells (to speed-up)
####
## Reference
## https://stackoverflow.com/questions/51621684/how-to-calculate-nucleus-amount-of-cell
## To get HSV values easily, use https://www.color-site.com/image_pickers
##
# Import modules
import numpy as np
import pandas as pd
import scipy
import cv2
import os
import datetime
#import pandas as pd
from matplotlib import pyplot as plt
from skimage.morphology import extrema
from skimage.segmentation import watershed as skwater
#from cv2_rolling_ball import subtract_background_rolling_ball
#from skimage.morphology import watershed as skwater
np.set_printoptions(threshold=np.inf)
pd.set_option('display.max_columns', 100)
# Import modules for date aquisition
#from PIL import Image
#from PIL.ExifTags import TAGS
# XML module
#import xml.etree.ElementTree as ET
#from oxdls import OMEXML
# Create output folder
output_folder="02_ParticleAnalysisOut"
os.makedirs(output_folder)
# Define ShowImage function
def ShowImage(title,img,ctype): # ----------------- #
if ctype=='bgr':
b,g,r = cv2.split(img) # get b,g,r
rgb_img = cv2.merge([r,g,b]) # switch it to rgb
plt.imshow(rgb_img)
elif ctype=='hsv':
rgb = cv2.cvtColor(img,cv2.COLOR_HSV2RGB)
plt.imshow(rgb)
elif ctype=='gray':
plt.imshow(img,cmap='gray')
elif ctype=='rgb':
plt.imshow(img)
else:
raise Exception("Unknown colour type")
plt.title(title)
plt.show()
# ------------------------------------------------- #
# --------------------------------------------------- #
# ------------------ Main loop ---------------------- #
# --------------------------------------------------- #
# User-defined paramters
calc_distribution_index = False
max_cell_area = 1000
min_cell_area = 60
#n_grid_row = 1
#n_grid_col = 1
# Define the number of clusters to find
#K = 2
#summary_out = pd.DataFrame()
image_folder = "data_image"
#image_folder_datetag = "2020-12-09T074433Z"
image_folder_bw = "01_BackgroundSubtrOut"
os.remove(image_folder_bw + "/.DS_Store") # Delete an unnecessary hidden file
for filename in np.sort(os.listdir(image_folder_bw + "/")):
# Read xml information
#filename='2021-03-17_23-39-34.jpg'
#filename='2021-03-17_08-59-07.jpg'
if filename[-3:]=="jpg":
image_date_str = filename[0:10] + " " + filename[11:13] + ":" + filename[14:16] + ":" + filename[17:19]
# Read in image
img0 = cv2.imread(image_folder + '/' + filename)
gray = cv2.imread(image_folder_bw + "/" + filename)
hsv = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)
#ShowImage('img', hsv,'gray')
# Convert pixel space to an array of triplets. These are vectors in 3-space.
Z = np.float32(hsv.reshape((-1,1)))
# Convert to floating point
#Z = np.float32(Z)
#ShowImage('img', Z, 'gray')
# Define the K-means criteria, these are not too important
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
# Perform the k-means transformation. What we get back are:
# Define the number of clusters to find
K = 2
#*Centers: The coordinates at the center of each 3-space cluster
#*Labels: Numeric labels for each cluster
#*Ret: A return code indicating whether the algorithm converged, &c.
ret, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
# Adjust center if center[1] is too small
# Because a too small value implies no or a small number of cells is in the image
if float(max(center)) < 80:
center[np.argmax(center)] = 80
label[Z >= (center[np.argmax(center)] - 60)] = np.argmax(center)
label[Z < center[np.argmax(center)] - 60] = np.argmin(center)
# Produce an image using only the center colours of the clusters
center = np.uint8(center)
khsv   = center[label.flatten()]
khsv   = khsv.reshape((hsv.shape))
# Reshape labels for masking
label = label.reshape(hsv.shape)
#ShowImage('img',label,'gray')
#ShowImage('img',khsv,'gray')
# Remove noise by eliminating single-pixel patches
khsv2 = np.uint8(khsv)
kernel  = np.ones((3,3),np.uint8)
opening = cv2.morphologyEx(khsv2, cv2.MORPH_OPEN, kernel)
opening = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)
# Normalize data
opening = (opening - opening.min()) / (opening.max() - opening.min())
opening = np.uint8(opening)
#ShowImage('img',opening,'gray')
# Identify areas which are surely foreground
h_fraction = 0.1
dist = cv2.distanceTransform(opening, cv2.DIST_L2, 5)
maxima = extrema.h_maxima(dist, h_fraction*dist.max())
# Dilate the maxima so we can see them
maxima_dilate = cv2.dilate(maxima, kernel, iterations=2)
# Finding unknown region
binary_w_maxima = cv2.subtract(opening, maxima_dilate)
#ShowImage('img',maxima_dilate,'gray')
# Marker labelling
ret, markers = cv2.connectedComponents(maxima_dilate)
# Add one to all labels so that sure background is 1, not 0.
markers = markers + 1
# Now, mark the region of unknown with zero
markers[binary_w_maxima==np.max(binary_w_maxima)] = 0
markers = skwater(-dist, markers, watershed_line = True)
markers_detected = markers.copy()
#ShowImage('img',markers[1150:1250,1450:1650],'gray')
# ------------------------------------------------------ #
# Single grid version -------------------------------- #
grid_n = 1
markers_grid_i = markers.copy()
markers_grid_i_detected = markers.copy()
#ShowImage('img',markers_grid_i_detected,'gray')
grid_cell_count = 0
grid_cell_area = 0
grid_cell_mean_area = 0
center_coord = np.empty((0,2), int)
if len(np.unique(markers_grid_i)) > 0:
for l in np.unique(markers_grid_i):
if l==0 or l==1: # Watershed line or Background
continue
else:
grid_cell_area_tmp = np.sum(markers_grid_i==l)
if grid_cell_area_tmp < max_cell_area and grid_cell_area_tmp > min_cell_area:
# Count total number of cells and calculate total cell area
grid_cell_area += grid_cell_area_tmp
grid_cell_count += 1
if not calc_distribution_index:
# Calculate the center of cell
center_i = np.mean(np.argwhere(markers_grid_i==l), axis = 0)
center_x = int(round(center_i[0]))
center_y = int(round(center_i[1]))
center_coord_tmp = np.array([center_x, center_y])
center_coord = np.vstack([center_coord, center_coord_tmp])
# Replace marker ID with -10 for visualization
markers_grid_i_detected[markers_grid_i==l] = -10
# Calculate mean cell area
if grid_cell_count > 0:
grid_cell_mean_area = grid_cell_area/grid_cell_count
else:
grid_cell_mean_area = np.nan
# Concatenate reconstructed marker grids
#markers_reconst0 = markers_grid_i_detected
# Calculate distance between centers
if grid_cell_count < 2 or not calc_distribution_index:
min_dist = min_dist_mean = min_dist_sd = min_dist_skew = min_dist_kurt = np.nan
else:
center_difs = np.expand_dims(center_coord, axis=1) - np.expand_dims(center_coord, axis=0)
center_dist = np.sqrt(np.sum(center_difs ** 2, axis=-1))
center_dist[np.eye(center_dist.shape[0], dtype = bool)] = np.nan
min_dist = np.nanmin(center_dist, axis = 1)
min_dist_mean = np.mean(min_dist)
min_dist_sd = np.std(min_dist)
#center_dist_skew = np.tril(center_dist)
min_dist_skew = scipy.stats.skew(min_dist)
min_dist_kurt = scipy.stats.kurtosis(min_dist)
# Summarize grid information
textFile = open(output_folder + "/0_results.txt","a")
textFile.write(filename +
"; date_time: " + image_date_str +
"; grid_n: {}".format(grid_n) +
"; count: {}".format(grid_cell_count) +
"; total_area: {}".format(grid_cell_area) +
"; mean_area: {:.5g}".format(grid_cell_mean_area) +
"; NNdist_mean: {:.4g}".format(min_dist_mean) +
"; NNdist_sd: {:.4g}".format(min_dist_sd) +
"; NNdist_skew: {:.4g}".format(min_dist_skew) +
"; NNdist_kurt: {:.4g}".format(min_dist_kurt) +
"\n")
textFile.close()
# Reshape markers_recombined
#markers_reconst = markers_reconst0[:,0:markers.shape[1]]
#for i in range(1,n_grid_col):
#  markers_reconst = np.vstack([markers_reconst, markers_reconst0[:,(markers.shape[1]*i):(markers.shape[1]*(i+1))]])
markers_reconst = markers_grid_i_detected
# Marked output image
imgout = img0.copy()
if grid_cell_count != 0:
imgout[markers == 0] = [255, 255, 255] # Label the watershed_line
imgout[markers_reconst == -10] = [100, 100, 255] # Fill the detected area
#ShowImage('imgout',imgout,'bgr')
cv2.imwrite(output_folder + "/" + filename[:-4] + "_analyzed.jpg", imgout)
# --------------------------------------------------- #
# --------------------- End ------------------------- #
# --------------------------------------------------- #
#result_summary_df.append(np.array([filename, image_date_str, grid_n, grid_cell_count,
#                           grid_cell_area, round(grid_cell_mean_area, 2)]))
#result_summary_df = pd.DataFrame(result_summary_df)
#result_summary_df = result_summary_df.rename(columns = {0:"filename", 1:"image_datetime",
#                                                       2:"grid_number", 3:"grid_cell_count",
#                                                        4:"grid_cell_total_area",
#                                                        5:"grid_cell_mean_area"})
# Summarize results
#textFile = open(output_folder + "/0_results.txt","a")
#textFile.write(filename + "; Date_time: " + image_date_str + "; Cell number: {}".format(cell_count) + "; Cell area: {:.5g}".format(cell_mean_area) + "\n")
#textFile.close()
# Summarize results
#result_summary = np.array([filename, round(cell_count, 0), round(cell_area, 0), round(cell_area/cell_count, 2)])
#result_summary_df = pd.DataFrame(result_summary.reshape(1,4))
#result_summary_df = result_summary_df.rename(columns = {0:"filename", 1:"cell_count", 2:"cell_total_area", 3:"cell_mean_area"})
#summary_out = summary_out.append(result_summary_df)
# Output summary
#summary_out = summary_out.reset_index(drop = True)
#summary_out.to_csv("%s/Summary_out.csv" % output_dir)
#summary_out.to_csv("data_jpg_analyzed/Summary_out.csv")
# ------------------------------------------------------ #
# ------------------------------------------------------ #
# ------------------------------------------------------ #
# Grid division version -------------------------------- #
if False:
# Calculate cell properties for each grid
grid_n = 1
for markers_row_i in np.array_split(markers, n_grid_row, axis = 0):
for markers_grid_i in np.array_split(markers_row_i, n_grid_col, axis = 1):
markers_grid_i_detected = markers_grid_i.copy()
#ShowImage('img',markers_grid_i_detected,'gray')
grid_cell_count = 0
grid_cell_area = 0
grid_cell_mean_area = 0
if len(np.unique(markers_grid_i)) > 0:
for l in np.unique(markers_grid_i):
if l==0 or l==1: # Watershed line or Background
continue
grid_cell_area_tmp = np.sum(markers_grid_i==l)
if grid_cell_area_tmp < max_cell_area and grid_cell_area_tmp > min_cell_area:
# Count total number of cells and calculate total cell area
grid_cell_area += grid_cell_area_tmp
grid_cell_count += 1
# Replace marker ID with -10 for visualization
markers_grid_i_detected[markers_grid_i==l] = -10
# Calculate mean cell area
if grid_cell_count > 0:
grid_cell_mean_area = grid_cell_area/grid_cell_count
# Concatenate reconstructed marker grids
if grid_n == 1:
markers_reconst0 = markers_grid_i_detected
else:
markers_reconst0 = np.hstack([markers_reconst0, markers_grid_i_detected])
# Calculate distance between maxima ponts
np.argwhere(img < 128)
# Summarize grid information
textFile = open(output_folder + "/0_results.txt","a")
textFile.write(filename +
"; date_time: " + image_date_str +
"; grid number: {}".format(grid_n) +
"; cell count: {}".format(grid_cell_count) +
"; cell total area: {}".format(grid_cell_area) +
"; cell mean area: {:.5g}".format(grid_cell_mean_area) +
"\n")
textFile.close()
# Add grid number
grid_n += 1
# ------------------------------------------------------ #
# ------------------------------------------------------ #
####
#### Real-time ERC: No.3 Visualize results
#### 2020.1.4 Ushio
#### 2020.3.8 Ushio, revised
#### 2020.3.19 Ushio, revised for 02_ParticleAnalysis_BW_v7 (v7)
#### 2020.4.12 Ushio, Add true input values (v8)
#### 2020.4.30 Ushio, Residuals devided by predicted values (v9)
####
# Load library
library(tidyverse); packageVersion("tidyverse") # 1.3.0, 2020.11.17
library(lubridate); packageVersion("lubridate") # 1.7.9, 2020.11.17
library(cowplot); packageVersion("cowplot") # 1.1.0, 2020.11.17
library(ggsci); packageVersion("ggsci") # 2.9, 2020.12.25
library(mgcv); packageVersion("mgcv") # 1.8.33, 2020.12.25
theme_set(theme_cowplot())
# Generate output folder
#od <- basename(rstudioapi::getSourceEditorContext()$path)
#(output_folder <- paste0(str_sub(od, end = -3), "Out")); rm(od)
output_folder <- "03_CompileDataOut"
dir.create(output_folder)
dir.create("00_SessionInfo")
# <---------------------------------------------> #
#           data source
# <---------------------------------------------> #
data_imageanalysis = "02_ParticleAnalysisOut/0_results.txt"
data_temperature = "data_temperature/20210511_temperature.txt"
data_input0 = "data_input/20210324_fish_original.csv"
# <---------------------------------------------> #
#           Load and compile cell data
# <---------------------------------------------> #
# Compile cell density data
compile_cell_data <- function(url_imagealaysis){
# Extract each variable
d_cell0 <- read.table(url_imagealaysis, sep = c(";"))
image_time <- ymd_hms(str_sub(d_cell0$V2, start = 13, end = 31))
grid_number <- as.factor(sapply(str_split(d_cell0$V3, pattern = " "), '[', 3))
cell_density <- as.numeric(sapply(str_split(d_cell0$V4, pattern = " "), '[', 3))
cell_area <- as.numeric(sapply(str_split(d_cell0$V6, pattern = " "), '[', 3))
cell_dist <- as.numeric(sapply(str_split(d_cell0$V7, pattern = " "), '[', 3))
cell_distsd <- as.numeric(sapply(str_split(d_cell0$V8, pattern = " "), '[', 3))
cell_distskew <- as.numeric(sapply(str_split(d_cell0$V9, pattern = " "), '[', 3))
cell_distkurt <- as.numeric(sapply(str_split(d_cell0$V10, pattern = " "), '[', 3))
# Compile the data as tibble
compiled_data <- tibble(time_cell = image_time,
grid_n = grid_number,
density = cell_density,
mean_area = cell_area,
nndist = cell_dist,
nndist_sd = cell_distsd,
nndist_skew = cell_distskew,
nndist_kurt = cell_distkurt)
# Return object
return(compiled_data)
}
d_cell <- compile_cell_data(data_imageanalysis)
#d_cell <- d_cell[150:nrow(d_cell),]
# <---------------------------------------------> #
#         Add temperature input data
#  (Record starts at the same time with time-lapse)
# <---------------------------------------------> #
d_input0 <- read_csv(data_input0)[1:256,]
# 5-times replication
d_cell <- d_cell %>% mutate(temperature_input = rep(d_input0$value, times = 2, each = 5)[1:nrow(.)])
# <---------------------------------------------> #
#         Load and compile temperature data
# <---------------------------------------------> #
# Load medium temperature data
d_temp0 <- read_csv(data_temperature, skip = 2, col_names = TRUE)[-1,c(1,3)]
colnames(d_temp0) <- c("time_temp", "temperature")
d_temp0$time_temp <- ymd_hms(d_temp0$time_temp)
# Write and re-load d_temperature to make col_types consistent
write_csv(d_temp0, sprintf("%s/d_temperature.csv", output_folder)); rm(d_temp0)
d_temperature <- read_csv(sprintf("%s/d_temperature.csv", output_folder),
col_types = cols(time_temp = col_datetime(format = ""))) %>% na.omit()
file.remove(sprintf("%s/d_temperature.csv", output_folder))
#d_temperature <- d_temperature[1:(nrow(d_temperature)-100),]
# <---------------------------------------------> #s
#     Combine and cell density and temperature
# <---------------------------------------------> #
dim(d_cell)
dim(d_temperature)
# Round time
#d_cell$date_time_round <- round_date(d_cell$date_time, unit = "minute")
# Identify the closest date_time (initial point)
min_df_id <- which.max(c(min(d_cell$time_cell), min(d_temperature$time_temp)))
if(min_df_id == 1){
# d_cell started first
time_dif_init <- abs(difftime(head(d_cell$time_cell, n = 1), d_temperature$time_temp, units = "mins"))
which.min(time_dif_init) ==  which(time_dif_init <= 0.5)[1] # Should be TRUE
init_id <- which(time_dif_init <= 0.5)[1]
# Calculate nrow
cell_nrow <- nrow(d_cell)
temp_nrow <- nrow(d_temperature[init_id:nrow(d_temperature),])
# Combine data
if(cell_nrow <= temp_nrow){
df_comb0 <- cbind(d_temperature[init_id:(init_id + cell_nrow - 1),], d_cell)
} else if (cell_nrow > temp_nrow){
df_comb0 <- cbind(d_temperature[init_id:nrow(d_temperature),], d_cell[1:temp_nrow,])
}
} else if (min_df_id == 2){
# d_temperature started first
time_dif_init <- abs(difftime(head(d_temperature$time_temp, n = 1), d_cell$time_cell, units = "mins"))
which.min(time_dif_init) ==  which(time_dif_init <= 0.5)[1] # Should be TRUE
init_id <- which(time_dif_init <= 0.5)[1]
# Calculate nrow
cell_nrow <- nrow(d_cell[init_id:nrow(d_cell),])
temp_nrow <- nrow(d_temperature)
# Combine data
if(cell_nrow <= temp_nrow){
df_comb0 <- cbind(d_temperature[1:cell_nrow,], d_cell[init_id:nrow(d_cell),])
} else if (cell_nrow > temp_nrow){
df_comb0 <- cbind(d_temperature, d_cell[init_id:(init_id + temp_nrow - 1),])
}
}
# Check time correspondence
all(difftime(df_comb0$time_cell, df_comb0$time_temp, units = "mins") < 0.5)
# Check colnames and re-compile
colnames(df_comb0)
df_comb0$time_rec <- round_date(df_comb0$time_temp, unit = "minute")
df_comb <- tibble(df_comb0[,c("time_rec", "temperature", "temperature_input", "density", "mean_area",
"nndist", "nndist_sd", "nndist_skew", "nndist_kurt")]) #%>% na.omit
#df_comb$date_time <- df_comb$date_time %>% ymd_hms()
rm(df_comb0)
dim(df_comb)
# Adjust NA
na_row <- apply(apply(df_comb, 2, is.na), 1, any)
df_comb[na_row,"nndist"] <- mean(df_comb$nndist, na.rm = T)
df_comb[na_row,"nndist_sd"] <- mean(df_comb$nndist_sd, na.rm = T)
df_comb[na_row,"nndist_skew"] <- mean(df_comb$nndist_skew, na.rm = T)
df_comb[na_row,"nndist_kurt"] <- mean(df_comb$nndist_kurt, na.rm = T)
# <---------------------------------------------> #
#            Remove long-term trend
#            and calculate residuals
# <---------------------------------------------> #
gam_res <- gam(density ~ s(as.numeric(time_rec)), data = df_comb)
gam_pred <- predict(gam_res)
gam_pred[gam_pred < 0] <- 0
df_comb$density_resid <- resid(gam_res)
#df_comb$area_resid <- gam(mean_area ~ s(as.numeric(time_rec)), data = df_comb) %>% resid()
#df_comb$nndist_resid <- gam(nndist ~ s(as.numeric(time_rec)), data = df_comb) %>% resid()
df_comb$density_rel_resid <- df_comb$density_resid/(gam_pred + 1)
df_comb$density_rel_resid10 <- df_comb$density_resid/(gam_pred + 10)
df_comb$density_dif <- as.numeric(c(NA, diff(df_comb$density, lag = 1)))
df_comb$density_rel_dif <- df_comb$density_dif/(gam_pred + 10)
# <---------------------------------------------> #
#            Visualize dynamics
# <---------------------------------------------> #
line_width = 0.3
g1 <- ggplot(df_comb, aes(x = time_rec, y = temperature_input)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_line(color = "red3", size = line_width) +
ylab(expression(paste("Temperature (", degree, "C)"))) +
xlab(NULL) +
NULL
g2 <- ggplot(df_comb, aes(x = time_rec, y = temperature)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_line(color = "red3", size = line_width) +
ylab(expression(paste("Temperature (", degree, "C)"))) +
xlab(NULL) +
NULL
g3 <- ggplot(df_comb, aes(x = time_rec, y = density)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_smooth(method = "gam", color = "red3", se = F, size = 0.3) +
geom_line(size = line_width) +
ylab("Cell density (/image)") +
xlab(NULL) +
NULL
g4 <- ggplot(df_comb, aes(x = time_rec, y = density_resid)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_hline(yintercept = 0, color = "red3", linetype = 2) +
geom_line(size = line_width) +
ylab("Cell density\nresiduals (/image)") +
xlab(NULL) +
NULL
g5 <- ggplot(df_comb, aes(x = time_rec, y = density_rel_resid)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_hline(yintercept = 0, color = "red3", linetype = 2) +
geom_line(size = line_width) +
ylab("GAM residuals \nper cell density + 1") +
xlab(NULL) +
NULL
g6 <- ggplot(df_comb, aes(x = time_rec, y = density_rel_resid10)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_line(size = line_width) +
ylab("GAM residuals \nper cell density + 10") +
xlab(NULL) +
NULL
g7 <- ggplot(df_comb, aes(x = time_rec, y = mean_area)) +
geom_vline(xintercept = seq(min(df_comb$time_rec), max(df_comb$time_rec), by = "30 mins"),
color = "gray", alpha = 0.7, size = 0.3) +
geom_line(size = line_width) +
ylab("Mean area") +
xlab(NULL) +
NULL
ggsave(sprintf("%s/dynamics.pdf", output_folder),
plot = plot_grid(g1, g2, g3, g4, g5, g6, g7, ncol = 2,
align = "hv", axis = "btlf", byrow = F),
width = 14, height = 9)
# <---------------------------------------------> #
#                 Save output
# <---------------------------------------------> #
# Standardize data
df_comb_std <- df_comb
df_comb_std[,2:ncol(df_comb_std)] <- apply(df_comb_std[,2:ncol(df_comb_std)], 2, function(x) as.numeric(scale(x)))
# Save output
write.csv(df_comb, sprintf("%s/data_all.csv", output_folder), row.names = F)
write.csv(df_comb_std, sprintf("%s/data_std_all.csv", output_folder), row.names = F)
# Save workspace and objects
save.image(sprintf("%s/%s.RData", output_folder, output_folder))
# Save session info
writeLines(capture.output(sessionInfo()),
sprintf("00_SessionInfo/%s_SessionInfo_%s.txt", output_folder, output_folder, substr(Sys.time(), 1, 10)))
# Check correlation between lagged and unlagged variables
cor_all <- c(NULL); lag_n_all <- 0:30
for(lag_n in lag_n_all){
cor_all <- c(cor_all,
cor(df_comb$temperature[seq(1,nrow(df_comb),5)+lag_n],
df_comb$temperature_input[seq(1,nrow(df_comb),5)], use = "complete.obs"))
}
names(cor_all) <- lag_n_all
plot(cor_all ~ lag_n_all, xlab = "Time-lag", ylab = "Correlation", type = "b")
names(cor_all)[which.max(cor_all)]
